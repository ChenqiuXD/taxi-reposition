import copy
import torch.nn as nn
import torch.optim
from agents.vdn.agent_q_function import AgentQFunction
from rl_algo.utils.base_agent import BaseAgent
import numpy as np

import os
from transition_model.train_model.model import Net
from torch_geometric.data import Data


class M_VDN(BaseAgent):
    """
    Trainer class for model-based VDN with MLP policies.
    """
    def __init__(self, args, env_config):
        super().__init__(args, env_config)

        self.share_policy = self.args.share_policy 
        self.edge_index = env_config["edge_index"]
        self.len_mat = env_config["len_mat"]
        self.dim_node_obs = env_config["dim_node_obs"]
        self.dim_edge_obs = env_config["dim_edge_obs"]
        self.episode_length = args.episode_length
        self.num_node = env_config["num_agents"]
        self.dim_overall_obs = self.dim_obs # including all nodes' features and edges' features

        # Initialize buffer
        self.buffer_size = self.args.buffer_size
        self.buffer_ptr = 0
        self.buffer = np.zeros((self.buffer_size, 2*self.dim_overall_obs+self.num_agents+2))
        # a transition is [s,a,r,s'], action is a [num_agent*1] vector, reward and time_step are both scalars

        # Initialize agent networks
        agent_q_config = {
            "num_node": env_config['num_agents'],
            "dim_action": env_config['dim_action'],
            "dim_node_obs": env_config['dim_node_obs']+1,   # 3+1, [idle drivers, upcoming cars, demands] + [time_step]
            "dim_edge_obs": env_config['dim_edge_obs'],
            "dim_message": 8,
            "out_channels": env_config['dim_action'],
            "message_hidden_layer": [256, 128],
            "update_hidden_layer": [128, 64],
        }

        if self.share_policy:
            self.agent_q_nets = [AgentQFunction(self.args, agent_q_config, id=0).to(self.device)]
        else:
            raise NotImplementedError("Not sharing policy for vdn method is not implemented, please change args.share_policy to true")

        # Initialize by xavier_uniform_
        for agent in self.agent_q_nets:
            for net in agent.modules():
                if isinstance(net, nn.Linear):
                    nn.init.normal_(net.weight, mean=0, std=0.05)
                    nn.init.zeros_(net.bias)

        # double dqn
        if self.args.use_double_q:
            print("Using double q learning")
            self.target_agent_q_nets = copy.deepcopy(self.agent_q_nets)
        else:
            raise NotImplementedError("Performance of not using double dqn is so poor thus is not implemented")

        # Collect all parameters
        self.parameters = []
        for agent in self.agent_q_nets:
            self.parameters += agent.parameters()

        self.optimizer = torch.optim.Adam(params=self.parameters, lr=self.lr, eps=self.opti_eps)
        # self.optimizer = torch.optim.SGD(params=self.parameters, lr=self.lr)
        self.loss_func = nn.MSELoss()

        # Model relevant parameters
        self.model_path = os.getcwd()+"\\transition_model\\train_model"
        self.model = self.load_transition_model(agent_q_config)
        try:
            self.num_gen_sample = self.args.generate_sample    # How many samples would be generated at each iteration. default: 1
        except:
            raise RuntimeError("Model-based RL should have argument generate_samples, yet it's not found in args. ")
        self.time_mats = np.zeros([self.episode_length, self.num_node, self.num_node])

        self.save_dir = os.path.abspath('.') + "\\run_this\\agents\\model_vdn\\"

    def learn(self):
        """Update the network parameters using Q-learning"""
        self.prep_train()
        # sample_batch_size = int(self.batch_size/(1+self.num_gen_sample))  # Data sampled from self.buffer
        sample_batch_size = 0   # All transition is generated by model

        if self.share_policy:
            sample_index = np.random.choice(self.buffer_size, self.batch_size, replace=False)
            batch_memory = self.buffer[sample_index, :]
            batch_state = batch_memory[:, :self.dim_overall_obs]
            batch_action = torch.LongTensor(
                batch_memory[:, self.dim_overall_obs: self.dim_overall_obs+self.num_agents]).to(self.device)
            batch_reward = batch_memory[:, self.dim_overall_obs+self.num_agents: self.dim_overall_obs+self.num_agents+1]
            batch_next_state = batch_memory[:, -self.dim_overall_obs-1:-1]
            # The last element is "time_step", therefore the "-1" in above line. 

            # Generate model_based data
            gen_batch_reward, gen_next_obs = self.generate_data(sample_index[sample_batch_size:])
            batch_reward[sample_batch_size:] = gen_batch_reward.reshape([len(sample_index)-sample_batch_size, 1])
            batch_next_state[sample_batch_size:, ] = gen_next_obs

            # Convert data to torch required data form
            batch_time_step = batch_memory[:, -1].reshape(-1,1)
            batch_state = self.obs2data(batch_state, batch_time_step).to(self.device)
            batch_next_state = self.obs2data(batch_next_state, batch_time_step+1).to(self.device)
            batch_reward = torch.FloatTensor(batch_reward).to(self.device)

            # Current Q values
            curr_agent_q_vals = self.get_q_value(batch_state, action=batch_action, is_target=False).view(self.batch_size, -1)
            print(curr_agent_q_vals)
            curr_mixed_q = torch.sum(curr_agent_q_vals, dim=1).view(-1, 1)

            # Next state Q values
            next_agent_q_vals_all = self.get_q_value(batch_next_state, is_target=True)
            next_agent_q_vals= torch.max(next_agent_q_vals_all, dim=1)[0].view(self.batch_size, self.num_agents)
            next_mixed_q = torch.sum(next_agent_q_vals, dim=1).view(-1, 1)

            # Judge whether it is the last step
            is_last_step = torch.LongTensor(batch_time_step+1==self.episode_length).to(self.device)
            next_mixed_q -= next_mixed_q*is_last_step

            # Compute Bellman loss
            target_mixed_q = (batch_reward + next_mixed_q).detach()
            loss = self.loss_func(curr_mixed_q, target_mixed_q)
            print("Current loss is: ", loss)

            # optimise
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
        else:
            raise NotImplementedError("Not sharing policy for vdn method is not implemented, please change args.share_policy to true")

    def get_q_value(self, batch_obs, action=None, is_target=False):
        """Get the batch q value of state-action pair
        :param: batch_obs is torch_geometric.data.Data type inputs
        :param: action is the batch of actions (dim: [batch_size, num_agents])
        :param: is_target defines whether get q values from target network
        :return: q_vals is the value given by current value function. 
                (batch_size*num_agent*dim_actions if action is not given; batch_size*num_agent otherwise)
        """
        # if isinstance(batch_obs, np.ndarray):   # Only used when choosing action
        #     batch_obs = torch.FloatTensor(batch_obs).to(self.device)
        if self.share_policy:
            if is_target:   # Target q networks
                q_vals = self.target_agent_q_nets[0](batch_obs) # batch_size*num_agents*dim_actions.
            else:   # Action q networks
                q_vals = self.agent_q_nets[0](batch_obs)    # batch_size*num*agents*dim_actions

            if action is not None:  # return q value of state-action pair
                action_batch = action.view(-1, 1)
                q_vals = torch.gather(q_vals, 1, action_batch)
        else:
            raise NotImplementedError("Not sharing policy for vdn method is not implemented, please change args.share_policy to true")
        return q_vals

    def append_transition(self, obs, action, reward, obs_, time_mat):
        """Store transition"""
        if self.buffer_ptr >= self.buffer_size:
            self.buffer_ptr = 0
        
        # Store time_mat
        self.time_mats[obs["time_step"]] = time_mat

        # Store buffer
        self.buffer[self.buffer_ptr] = np.concatenate((self.obs2array(obs), action, [reward], self.obs2array(obs_), [obs["time_step"]]))
        self.buffer_ptr += 1

    def choose_action(self, obs, is_random=False):
        """
        Choose action according to obs.
        :param obs: observation (dict) with ["]
        :param is_random: (bool) whether randomly choose action
        """
        eps = np.random.uniform(0, 1)
        if is_random or eps >= self.e_greedy:
            return [np.random.choice(range(self.dim_action)) for _ in range(self.num_agents)]
        else:
            if self.share_policy:
                data = self.obs2data(obs)   # Convert obs(dict) to agent's network's input data(torch_geometric.Data)
                q_vals = self.agent_q_nets[0](data) # 5(num_nodes)*5(num_actions) represents the state-action values
                actions = torch.max(q_vals, dim=1)[1].tolist()
            else:
                raise NotImplementedError("Not sharing policy for vdn method is not implemented, please change args.share_policy to true")
            return actions

    def generate_data(self, data_idx):
        """This function generate data based on self.model which outputs the equlibrium actions of node
        @params: 
            data_idx: (list) a list of sampled data_idx which would be used to predict the transition using model
        @returns:
            batch_next_state: (ndarray, [self.dim_overall_obs,len(data_idx)]) model prediction of next state
            overall_cost: (FloatTensor, [1*len(data_idx)]) calculated cost based on model prediction
        """
        batch_data = self.buffer[data_idx]
        gen_batch_size = len(data_idx)
        assert gen_batch_size>=2, ("generate data should bigger than 1, please modify batch_size or generate_sample")

        # Convert observation and action to GNN input (cannot use obs2data since there includes action)
        obs = batch_data[:, :self.dim_overall_obs]
        batch_action = batch_data[:, self.dim_overall_obs : self.dim_overall_obs+self.num_agents]
        x = torch.FloatTensor(np.concatenate(np.concatenate(
                [obs[:, :self.dim_node_obs*self.num_agents], batch_action], axis=1).\
                reshape(-1, self.dim_node_obs+1, self.num_agents).swapaxes(1,2)))
        node_state_end_idx = self.dim_node_obs*self.num_agents
        tmp = obs[:, node_state_end_idx:]
        tmp_edge_attr = np.concatenate(tmp.reshape(-1, self.dim_edge_obs, self.edge_index.shape[1]).swapaxes(1,2))
        edge_attr = torch.FloatTensor(np.vstack([tmp_edge_attr[:,1], tmp_edge_attr[:,0]/10000.0*500.0]).swapaxes(0,1))
        data_edge_index = np.concatenate([[self.edge_index+i*self.num_agents]\
                                            for i in range(gen_batch_size)]).swapaxes(0,1).reshape(self.dim_edge_obs,-1)
        gen_data = Data(x=x, edge_attr=edge_attr, edge_index=torch.LongTensor(data_edge_index))

        nodes_action_prob = self.model(gen_data).detach().cpu().numpy().reshape(-1, self.num_agents, self.num_agents)
        nodes_action_prob /= 100.0
        idle_drivers = obs[:, :self.num_agents] # idle_drivers at last step
        nodes_action = np.vstack([ (nodes_action_prob[i].T*idle_drivers[i]).T for i in range(nodes_action_prob.shape[0]) ]).reshape(-1, self.num_agents, self.num_agents)

        # Calc cost
        overall_cost = np.zeros(len(data_idx))
        for i in range(len(data_idx)):
            normalization_factor = {"idle_cost": 3, "travelling_cost": 1, "bonus_cost": 1}
            action = nodes_action[i]
            bonuses = batch_action[i]
            batch_time_step = batch_data[:, -1].astype(int)  # The last element in buffer is "time_step"
            time_mat = self.time_mats[batch_time_step[i]]
            upcoming_cars = obs[i][self.num_agents:self.num_agents*2]
            demands = obs[i][self.num_agents*2:self.num_agents*3]

            # Calculate idle/demand cost using mse loss
            node_all_cars = np.sum(action, axis=0) + upcoming_cars
            nodes_distribution = node_all_cars / np.sum(node_all_cars)
            demands_distribution = demands / np.sum(demands)
            
            # Idle_cost
            idle_cost = np.sqrt(np.sum((nodes_distribution-demands_distribution)**2))
            idle_cost *= normalization_factor['idle_cost']

            # Calculate the travelling time
            travelling_nodes = action*(time_mat!=0)  # Eliminate the drivers who stay at current nodes. 
            max_time = np.sum( np.sum(travelling_nodes, axis=1)*np.max(time_mat, axis=1) )
            min_time = np.sum( np.sum(travelling_nodes, axis=1)*np.min(time_mat+(time_mat==0)*1e3, axis=1) )
            avg_travelling_cost = (np.sum(action*time_mat)-min_time) / (max_time-min_time)
            avg_travelling_cost *= normalization_factor['travelling_cost']

            # Calculate bonuses
            bonuses_cost = np.sum(action*bonuses)/(np.sum(action)*self.dim_action)
            bonuses_cost *= normalization_factor['bonus_cost']

            # Calculate comprehensive cost
            overall_cost[i] = -(0.4*idle_cost + 0.4*avg_travelling_cost + 0.2*bonuses_cost)*100
            
        # Compute next state (only changes idle_drivers of obs_ since upcoming_cars, demands, traffic remains the same)
        idle_drivers = np.maximum(0, np.array([np.sum(nodes_action[i], axis=0) for i in range(nodes_action.shape[0])])   # re-positioned drivers
                                     +batch_data[:, self.num_agents: self.num_agents*2]     # plus upcoming cars
                                     -batch_data[:, self.num_agents*2:self.num_agents*3]).astype(int)   # minus demands
        batch_next_state = self.buffer[data_idx, -self.dim_overall_obs-1:-1]
        batch_next_state[:, :self.num_agents] = idle_drivers

        return overall_cost, batch_next_state

    def obs2array(self, obs):
        """This function transform state in dict form to numpy array (n_agents*dim_node_obs+n_edges*dim_edge_obs)
        @params:
            obs: observation (dict / a list of dict) 
        @ return:
            state: state represent by np.ndarray
        """
        if type(obs)==dict: # When obs is just one state
            state = np.concatenate([obs["idle_drivers"],  
                                    obs["upcoming_cars"],
                                    obs["demands"], 
                                    np.array([obs["edge_traffic"][self.edge_index[0,j], self.edge_index[1,j]] for j in range(self.edge_index.shape[1])]), 
                                    self.len_mat])
        else:   # When obs is a batch of states
            state = np.vstack([np.concatenate([obs[i]["idle_drivers"], 
                                               obs[i]["upcoming_cars"], 
                                               obs[i]["demands"], 
                                               np.array([obs["edge_traffic"][self.edge_index[0,j], self.edge_index[1,j]] for j in range(self.edge_index.shape[1])]), 
                                               self.len_mat]) for i in range(len(obs))])
        return state

    def obs2data(self, obs, batch_time_step=-1):
        """This function convert obs(dict) to network's input data(torch_geometric.Data)
        @params: 
            obs: observation (dict/np.ndarray) (check self.obs2array() to find out its contents if it's dict) 
        @return:
            data: distributed agents' obervations, in torch_geometric.Data form
        """
        if type(obs) == dict:   # obs is a single dict object
            x = torch.FloatTensor(np.vstack([obs["idle_drivers"], obs["upcoming_cars"], obs["demands"], [obs["time_step"]]*self.num_agents]).T)
            edge_attr = torch.FloatTensor(np.vstack([np.array([obs["edge_traffic"][self.edge_index[0,j], self.edge_index[1,j]] for j in range(self.edge_index.shape[1])]), 
                                                     self.len_mat]).T)
            data = Data(x=x, edge_attr=edge_attr, edge_index=torch.LongTensor(self.edge_index))
        elif type(obs)==np.ndarray:
            assert (batch_time_step!=-1).all(), ("Batch time step equals -1 in obs2data function")
            if len(obs.shape)==1:   # obs contains a single observation
                raise NotImplementedError("training batch data should not be 0, plesae change batch size to more than 1")
            else:   # obs is a batch of observation
                batch_size = obs.shape[0]
                x = torch.FloatTensor(np.concatenate(
                        np.concatenate((obs[:, :self.dim_node_obs*self.num_agents],
                                        np.hstack([[step_i]*self.num_agents for step_i in batch_time_step]).T),
                                        axis=1).reshape(-1, self.dim_node_obs+1, self.num_agents).swapaxes(1,2)))
                node_state_end_idx = self.dim_node_obs*self.num_agents
                edge_attr = torch.FloatTensor(np.concatenate(
                                obs[:, node_state_end_idx : ].\
                                reshape(-1, self.dim_edge_obs, self.edge_index.shape[1]).swapaxes(1,2)))
                data_edge_index = np.concatenate([[self.edge_index+i*self.num_agents]\
                                                   for i in range(batch_size)]).swapaxes(0,1).reshape(self.dim_edge_obs,-1)
                data = Data(x=x, edge_attr=edge_attr, edge_index=torch.LongTensor(data_edge_index))
        return data
        
    def load_transition_model(self, agent_q_config):
        """Load transition model learned from transition model step (a gnn used to approximate the reaction of drivers in equlibrium)"""
        # Instantiate a model
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        config = {
            # For making decision, the obs is 3. When predicting transition, we add actions, therefore the in_channels should plus 1
            "in_channels": agent_q_config["dim_node_obs"],    # [idle drivers, upcoming cars, demands] + [bonuses]
            "edge_channels": agent_q_config["dim_edge_obs"], 
            "out_channels": agent_q_config["out_channels"], 
            "dim_message": 8,
            "message_hidden_layer": [128, 64],
            "update_hidden_layer": [64,32],
        }
        model = Net(agent_q_config["num_node"], config, device, flow='source_to_target').to(device)

        # Load parameters
        model.load_state_dict(torch.load(self.model_path+"\\tran_model.pkl"))
        return model

    def hard_target_update(self):
        print("Hard update targets")
        for i in range(len(self.agent_q_nets)):
            self.target_agent_q_nets[i].load_state_dict(self.agent_q_nets[i].state_dict())

    def prep_train(self):
        """Used to net.train()"""
        for agent in self.agent_q_nets:
            agent.train()

    def prep_eval(self):
        """Used to net.eval()"""
        for agent in self.agent_q_nets:
            agent.eval()

    def save_network(self):
        """Save network to test directory"""
        import datetime
        for idx, agent in enumerate(self.agent_q_nets):
            file_name = "net_"+str(idx)+"_"+datetime.datetime.now().strftime('%m%d_%H%M')
            torch.save(agent.state_dict(), self.save_dir+file_name+".pkl")
            print("Q-network saved in ", self.save_dir)

    def restore(self):
        for idx, agent in enumerate(self.agent_q_nets):
            path = self.save_dir + "\\net_"+str(idx)+".pkl"
            agent.load_state_dict(torch.load(path))
            print("Succesfully loaded q-network")
